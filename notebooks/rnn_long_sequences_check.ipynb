{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 41)\n",
      "(476, 11)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = Path('..') / 'data'\n",
    "\n",
    "train_df = ps.read_csv(data_dir / 'train.csv')\n",
    "test_df = ps.read_csv(data_dir / 'test.csv')\n",
    "\n",
    "\n",
    "class AvgModel:\n",
    "    def __init__(self, *models):\n",
    "        self.models = [torch.jit.load(str(m), map_location=device) for m in models]\n",
    "        for model in self.models:\n",
    "            model = model.eval()\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        outputs = []\n",
    "        for m in self.models:\n",
    "            out = m(*inputs)\n",
    "            out = torch.sigmoid(out)\n",
    "            outputs.append(out)\n",
    "        res = torch.stack(outputs, dim=0)\n",
    "        return torch.mean(res, 0)\n",
    "        \n",
    "\n",
    "model = AvgModel(\n",
    "    Path('..') / 'lstm_gru_attn_embs_0.pt',  # 0.3940\n",
    "    Path('..') / 'lstm_gru_attn_embs_1.pt',  # 0.3908\n",
    ")\n",
    "\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "    'question_asker_intent_understanding',\n",
    "    'question_body_critical',\n",
    "    'question_conversational',\n",
    "    'question_expect_short_answer',\n",
    "    'question_fact_seeking',\n",
    "    'question_has_commonly_accepted_answer',\n",
    "    'question_interestingness_others',\n",
    "    'question_interestingness_self',\n",
    "    'question_multi_intent',\n",
    "    'question_not_really_a_question',\n",
    "    'question_opinion_seeking',\n",
    "    'question_type_choice',\n",
    "    'question_type_compare',\n",
    "    'question_type_consequence',\n",
    "    'question_type_definition',\n",
    "    'question_type_entity',\n",
    "    'question_type_instructions',\n",
    "    'question_type_procedure',\n",
    "    'question_type_reason_explanation',\n",
    "    'question_type_spelling',\n",
    "    'question_well_written',\n",
    "    'answer_helpful',\n",
    "    'answer_level_of_information',\n",
    "    'answer_plausible',\n",
    "    'answer_relevance',\n",
    "    'answer_satisfaction',\n",
    "    'answer_type_instructions',\n",
    "    'answer_type_procedure',\n",
    "    'answer_type_reason_explanation',\n",
    "    'answer_well_written'\n",
    "]\n",
    "\n",
    "text_columns = [\"question_title\", \"question_body\", \"answer\", \"category\", \"host\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences: list,\n",
    "                  max_len: int,\n",
    "                  value: int = 0,\n",
    "                  padding: str = \"pre\",\n",
    "                  dtype=np.int32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pad sequences with specified value.\n",
    "    \n",
    "    Example of different padding strategies:\n",
    "\n",
    "    >>> seqs = [[1, 2, 3], [4, 5], [6]]\n",
    "    >>> pad_sequences(seqs, max_len=3, padding=\"post\")\n",
    "    array([[1, 2, 3],\n",
    "       [4, 5, 0],\n",
    "       [6, 0, 0]], dtype=int32)\n",
    "    >>> pad_sequences(seqs, max_len=3, padding=\"pre\")\n",
    "    array([[1, 2, 3],\n",
    "       [0, 4, 5],\n",
    "       [0, 0, 6]], dtype=int32)\n",
    "    \"\"\"\n",
    "\n",
    "    if not max_len > 0:\n",
    "        raise ValueError(\"`max_len` should be greater than 0\")\n",
    "\n",
    "    if padding not in {\"pre\", \"post\"}:\n",
    "        raise ValueError(\"`padding` should be one of `pre` or `post`\")\n",
    "\n",
    "    features = np.full(\n",
    "        shape=(len(sequences), max_len),\n",
    "        fill_value=value,\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "    for idx, row in enumerate(sequences):\n",
    "        if len(row):\n",
    "            if padding == \"pre\":\n",
    "                features[idx, -len(row):] = np.array(row)[:max_len]\n",
    "            else:\n",
    "                features[idx, : len(row)] = np.array(row)[:max_len]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class DummyTokenizer:\n",
    "    def __init__(self,\n",
    "                 index2word,\n",
    "                 index2host,\n",
    "                 index2category,\n",
    "                 text_fields,\n",
    "                 host_field,\n",
    "                 category_field,\n",
    "                 unknown_token = \"<unk>\"):\n",
    "        self.idx2word = index2word\n",
    "        self.word2idx = {w: idx for idx, w in enumerate(index2word)}\n",
    "\n",
    "        self.idx2host = index2host\n",
    "        self.host2idx = {h: idx for idx, h in enumerate(index2host)}\n",
    "\n",
    "        self.idx2category = index2category\n",
    "        self.category2idx = {c: idx for idx, c in enumerate(index2category)}\n",
    "\n",
    "        self.text_fields = text_fields\n",
    "        self.host_field = host_field\n",
    "        self.category_field = category_field\n",
    "\n",
    "        self.separate_chars = [\n",
    "            ',', '.', '\"', ':', ')', '(', '-', '!', '?', \n",
    "            '|', ';', \"'\", '$', '&', '/', '[', ']', '>', \n",
    "            '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', \n",
    "            '£', '·', '_', '{', '}', '©', '^', '®', '`',\n",
    "            '<', '→', '°', '€', '™', '›',  '♥', '←', '×', \n",
    "            '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', \n",
    "            '\\xa0', '\\t', '“', '★', '”', '–', '●', 'â', \n",
    "            '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±',\n",
    "            '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—',\n",
    "            '‹', '─', '\\u3000', '\\u202f', '▒', '：', '¼', \n",
    "            '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', \n",
    "            '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', \n",
    "            '¾', 'Ã', '⋅', '‘', '∞', '«', '∙', '）', '↓', \n",
    "            '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', \n",
    "            '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', \n",
    "            '¹', '≤', '‡', '√', \n",
    "        ]\n",
    "        self.lower = True\n",
    "        self.split = \" \"\n",
    "        self.UNK = unknown_token\n",
    "\n",
    "    def tokenize(self, state: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Return tokenized (for each field: str -> list[str]) state\n",
    "        \"\"\"\n",
    "        for txt_field in self.text_fields:\n",
    "            s = state[txt_field]\n",
    "\n",
    "            if self.lower:\n",
    "                s = s.lower()\n",
    "\n",
    "            s = re.sub('[0-9]{5,}', '#####', s)\n",
    "            s = re.sub('[0-9]{4}', '####', s)\n",
    "            s = re.sub('[0-9]{3}', '###', s)\n",
    "            s = re.sub('[0-9]{2}', '##', s)\n",
    "\n",
    "            for c in self.separate_chars:\n",
    "                s = s.replace(c, f\" {c} \")\n",
    "            \n",
    "            state[txt_field] = s\n",
    "        for field in (self.host_field, self.category_field):\n",
    "            state[field] = [state[field]]\n",
    "        return state\n",
    "    \n",
    "    def convert_tokens_to_ids(self, state: dict) -> dict:\n",
    "        for txt_field in self.text_fields:\n",
    "            state[txt_field] = [self.word2idx[token if token in self.word2idx else self.UNK] \n",
    "                                for token in state[txt_field]]\n",
    "        \n",
    "        state[self.host_field] = [self.host2idx[host if host in self.host2idx else self.UNK] \n",
    "                                  for host in state[self.host_field]]\n",
    "        state[self.category_field] = [self.category2idx[category if category in self.category2idx else self.UNK] \n",
    "                                      for category in state[self.category_field]]\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def from_file(tokenizer_dir):\n",
    "        with open(tokenizer_dir, 'r') as f:\n",
    "            content = json.load(f)\n",
    "        \n",
    "        return DummyTokenizer(\n",
    "            index2word=content[\"text\"],\n",
    "            index2host=content[\"host\"],\n",
    "            index2category=content[\"category\"],\n",
    "            text_fields=[\"question_title\", \"question_body\", \"answer\"],\n",
    "            host_field=\"host\",\n",
    "            category_field=\"category\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedFieldsDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 df: DataFrame, \n",
    "                 feature_cols: List[str], \n",
    "                 target: List[str], \n",
    "                 tokenizer_dir: str, \n",
    "                 field: str = None):\n",
    "        self.df: DataFrame = df\n",
    "        self.features: List[str] = feature_cols\n",
    "        self.target: List[str] = target\n",
    "        self.tokenizer: DummyTokenizer = DummyTokenizer.from_file(tokenizer_dir)\n",
    "        self.field = field\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        index = self.df.index[idx]\n",
    "        state = {c: self.df.at[index, c] for c in self.features}\n",
    "        state = self.tokenizer.tokenize(state)               # split strings to lists of tokens\n",
    "        state = self.tokenizer.convert_tokens_to_ids(state)  # map tokens to ids\n",
    "        target = [self.df.at[index, c] for c in self.target] \n",
    "        return state, target\n",
    "\n",
    "\n",
    "class FieldsCollator:\n",
    "    def __init__(self, \n",
    "                 fields: list,\n",
    "                 ignore_fields: list = None,\n",
    "                 is_test: bool = False, \n",
    "                 percentile: int = 100, \n",
    "                 max_len: int = 500):\n",
    "        self.fields = fields\n",
    "        ignore_fields = {} if not ignore_fields else set(ignore_fields)\n",
    "        self.ignore_fields = ignore_fields\n",
    "        self.is_test = is_test\n",
    "        self.percentile = percentile\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.is_test:\n",
    "            sequences = batch\n",
    "        else:\n",
    "            sequences, labels = zip(*batch)\n",
    "\n",
    "        res = {}\n",
    "        for f in self.fields:\n",
    "            seq = [item[f] for item in sequences]\n",
    "            lengths = np.array(list(map(len, seq)))\n",
    "            max_len = int(np.percentile(lengths, self.percentile))\n",
    "            if f not in self.ignore_fields:\n",
    "                max_len = min(int(np.percentile(lengths, self.percentile)), self.max_len)\n",
    "            seq = torch.from_numpy(pad_sequences(seq, max_len, padding='post'))\n",
    "            seq = seq.long()\n",
    "            res[f] = seq\n",
    "\n",
    "        if self.is_test:\n",
    "            return res\n",
    "        else:\n",
    "            res[\"targets\"] = torch.FloatTensor(labels)\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"../data/uncased_L-12_H-768_A-12\")\n",
    "\n",
    "\n",
    "def combined_len(title, body, answer):\n",
    "    title_body = bert_tokenizer.tokenize(title + \",\" + body)\n",
    "    ans = bert_tokenizer.tokenize(answer)\n",
    "    tokens = [\"[CLS]\"] + title_body + [\"[SEP]\"] + ans + [\"[SEP]\"]\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "def field_len(feature):\n",
    "    tokens = bert_tokenizer.tokenize(feature)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df[\"sequences_len\"] = df.apply(lambda row: combined_len(row[\"question_title\"], row[\"question_body\"], row[\"answer\"]), axis=1)\n",
    "    df[\"title_len\"] = df[\"question_title\"].apply(field_len)\n",
    "    df[\"body_len\"] = df[\"question_body\"].apply(field_len)\n",
    "    df[\"answer_len\"] = df[\"answer\"].apply(field_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_seqs_train_df = train_df[train_df[\"sequences_len\"] > 512]\n",
    "short_seqs_train_df = train_df[train_df[\"sequences_len\"] <= 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr(trues, preds):\n",
    "    rhos = []\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        rhos.append(spearmanr(col_trues, col_pred).correlation)\n",
    "    return np.mean(rhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking performance on long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shapes - (1628, 30)\n"
     ]
    }
   ],
   "source": [
    "dataset = TokenizedFieldsDataset(\n",
    "    long_seqs_train_df, \n",
    "    feature_cols=text_columns,\n",
    "    target=targets,\n",
    "    tokenizer_dir=\"../data/vocab.json\",\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=FieldsCollator(\n",
    "        fields=text_columns,\n",
    "        ignore_fields=[\"category\", \"host\"],\n",
    "        max_len=1500,\n",
    "        percentile=90,\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "num_batches = len(loader)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, tensors in enumerate(loader):\n",
    "        title = tensors[\"question_title\"].to(device)\n",
    "        body = tensors[\"question_body\"].to(device)\n",
    "        ans = tensors[\"answer\"].to(device)\n",
    "        cat = tensors[\"category\"].to(device)\n",
    "        host = tensors[\"host\"].to(device)\n",
    "        out = model(title, body, ans, cat, host)\n",
    "        out = out.detach().cpu().numpy()\n",
    "        preds.append(out)\n",
    "        print(f'{batch_idx + 1:4d}/{num_batches}', end='\\r')\n",
    "\n",
    "preds = np.vstack(preds)\n",
    "print(f'Test shapes - {preds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (1628, 30) (1628, 30)\n"
     ]
    }
   ],
   "source": [
    "print('Shapes', preds.shape, long_seqs_train_df[targets].values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4250857888299656"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_spearmanr(long_seqs_train_df[targets].values, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shapes - (4451, 30)\n"
     ]
    }
   ],
   "source": [
    "dataset = TokenizedFieldsDataset(\n",
    "    short_seqs_train_df, \n",
    "    feature_cols=text_columns,\n",
    "    target=targets,\n",
    "    tokenizer_dir=\"../data/vocab.json\",\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=FieldsCollator(\n",
    "        fields=text_columns,\n",
    "        ignore_fields=[\"category\", \"host\"],\n",
    "        max_len=1500,\n",
    "        percentile=90,\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "num_batches = len(loader)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, tensors in enumerate(loader):\n",
    "        title = tensors[\"question_title\"].to(device)\n",
    "        body = tensors[\"question_body\"].to(device)\n",
    "        ans = tensors[\"answer\"].to(device)\n",
    "        cat = tensors[\"category\"].to(device)\n",
    "        host = tensors[\"host\"].to(device)\n",
    "        out = model(title, body, ans, cat, host)\n",
    "        out = out.detach().cpu().numpy()\n",
    "        preds.append(out)\n",
    "        print(f'{batch_idx + 1:4d}/{num_batches}', end='\\r')\n",
    "\n",
    "preds = np.vstack(preds)\n",
    "print(f'Test shapes - {preds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (4451, 30) (4451, 30)\n"
     ]
    }
   ],
   "source": [
    "print('Shapes', preds.shape, short_seqs_train_df[targets].values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406158437019835"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_spearmanr(short_seqs_train_df[targets].values, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
