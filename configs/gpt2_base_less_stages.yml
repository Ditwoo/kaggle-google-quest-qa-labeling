model_params:
  model: PTM
  pretrain_dir: gpt2
  num_classes: 30
  pad_token: 5

# distributed_params:
#   opt_level: O1
#   syncbn: True

stages:
  state_params:
    main_metric: spearman
    minimize_metric: False

  data_params:
    num_workers: &nw 6
    batch_size: *nw
    tokenizer: gpt2

  criterion_params:
    criterion: BCEWithLogitsLoss

  callbacks_params:
    loss:
      callback: CriterionCallback
      input_key: targets

    optim:
      callback: OptimizerCallback
      # accumulation_steps: 16

    spearman:
      callback: SpearmanScoreCallback
      classes: 30

    saver:
      callback: CheckpointCallback

    early_stopping:
      callback: EarlyStoppingCallback
      patience: 2
      metric: spearman
      minimize: False
    
    telegram_logger:
      callback: TelegramLogger
      log_on_stage_start: False
      log_on_loader_start: False

  stage1:
    state_params:
      num_epochs: 10
    
    optimizer_params:
      optimizer: RAdam
      lr: 0.00001
      betas: &betas [0.9, 0.98]
      eps: &eps 0.000000001
      # weight_decay: &wd 0.001

  stage2:
    state_params:
      num_epochs: 5
    
    optimizer_params:
      optimizer: RAdam
      lr: 0.000001
      betas: *betas
      # eps: *eps
      # weight_decay: *wd

  stage3:
    state_params:
      num_epochs: 5
    
    optimizer_params:
      optimizer: RAdam
      lr: 0.0000001
      betas: *betas
      eps: *eps
      # weight_decay: *wd
