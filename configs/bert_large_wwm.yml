model_params:
  model: PooledTransfModel
  pretrain_dir: bert-large-uncased-whole-word-masking
  num_classes: 30

# distributed_params:
#   opt_level: O1
#   syncbn: True

stages:
  state_params:
    main_metric: spearman
    minimize_metric: False

  data_params:
    num_workers: &nw 2
    batch_size: *nw
    transformer_dir: data/uncased_L-24_H-1024_A-16

  criterion_params:
    criterion: BCEWithLogitsLoss

  callbacks_params:
    loss:
      callback: CriterionCallback
      input_key: targets

    optim:
      callback: OptimizerCallback
      accumulation_steps: 4

    spearman:
      callback: SpearmanScoreCallback
      classes: 30

    saver:
      callback: CheckpointCallback

  stage1:
    state_params:
      num_epochs: 10
    
    optimizer_params:
      optimizer: Adam
      lr: 0.00003

  # stage2:
  #   state_params:
  #     num_epochs: 10
    
  #   optimizer_params:
  #     optimizer: Adam
  #     lr: 0.00001

  # stage3:
  #   state_params:
  #     num_epochs: 10
    
  #   optimizer_params:
  #     optimizer: Adam
  #     lr: 0.000005
