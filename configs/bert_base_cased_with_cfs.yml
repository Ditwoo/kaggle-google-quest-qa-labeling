model_params:
  model: PTM
  pretrain_dir: bert-base-cased
  num_classes: 30
  pad_token: 0

# distributed_params:
#   opt_level: O1
#   syncbn: True

stages:
  state_params:
    main_metric: spearman
    minimize_metric: False

  data_params:
    num_workers: &nw 6
    batch_size: *nw
    tokenizer: bert-base-cased

  criterion_params:
    criterion: BCEWithLogitsLoss

  callbacks_params:
    loss:
      callback: CriterionCallback
      input_key: targets

    optim:
      callback: OptimizerCallback
      # accumulation_steps: 16

    spearman:
      callback: SpearmanScoreCallback
      classes: 30

    saver:
      callback: CheckpointCallback

    early_stopping:
      callback: EarlyStoppingCallback
      patience: 3
      metric: spearman
      minimize: False

  stage1:
    state_params:
      num_epochs: 10
    
    optimizer_params:
      optimizer: Adam
      lr: 0.00003

  stage2:
    state_params:
      num_epochs: 5
    
    optimizer_params:
      optimizer: Adam
      lr: 0.00001 # 0.000005
  
  stage3:
    state_params:
      num_epochs: 5
    
    optimizer_params:
      optimizer: Adam
      lr: 0.000001
  
  stage4:
    state_params:
      num_epochs: 5
    
    optimizer_params:
      optimizer: Adam
      lr: 0.0000001
